% #####################################################################
% #####################################################################
% ##                                                                 ##
% ##                             Lizenz:                             ##
% ##                         CC BY-NC-SA 3.0                         ##
% ##      http://creativecommons.org/licenses/by-nc-sa/3.0/de/       ##
% ##                                                                 ##
% #####################################################################
% ##   Diese Datei kann beliebig verändert werden, solange darauf    ##
% ##     hingewiesen wird, dass dieses Dokument ursprünglich von     ##
% ##                                                                 ##
% ##                        www.ei-studium.de                        ##
% ##                                                                 ##
% ##                             stammt.                             ##
% ## Dies gilt insbesondere auch für alle daraus erstellten Dateien. ##
% ##    Des Weiteren muss die Weitergabe dieser Dateien unter der    ##
% ##                    gleichen Lizenz erfolgen.                    ##
% #####################################################################
% #####################################################################
\documentclass[a4paper,twocolumn,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage[top=2.0cm,bottom=1.5cm,left=1.0cm,right=1.0cm]{geometry}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{sectsty}
\usepackage{colortbl}
\usepackage{cancel}
\usepackage{listings}
\usepackage{color}
\usepackage{amsmath}
\usepackage{epstopdf}
\usepackage{fancyhdr}
\usepackage[pdfborder={0 0 0}]{hyperref}

\setlist{itemsep=.01mm}
\setenumerate{label=\emph{\arabic*})}
\setlength{\columnsep}{1cm}
\parindent 0mm

\partfont{\Large}
\sectionfont{\large \sc\bf}
\subsectionfont{\normalsize}
\subsubsectionfont{\small\textit}

\pagestyle{fancy}
\lhead[\leftmark]{Formelsammlung Mathematik 1 für EI}
\chead[\leftmark]{\url{http://www.ei-studium.de}}
\rhead[\leftmark]{Erstelldatum: \today}
\lfoot[\leftmark]{Keine Garantie auf Vollständigkeit und Richtigkeit!}
\cfoot[\leftmark]{}
\rfoot[\leftmark]{\thepage}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0.5pt}

\begin{document}
\tableofcontents
\newpage

\part{Mathematik 1}

\section{Grundlagen}

\subsection{Vollständige Induktion}
\textbf{Induktionsanfang:}\\
Behauptung mit kleinstem Element für n testen.\\
\textbf{Induktionsschritt:}\\
Behauptung mit $n+1$ testen (Behauptung nutzen!).\\
\textbf{Induktionsschluss:}\\
Schlussfolgerung aus dem Ergebnis ziehen.

\subsection{Summen}
Distributivgesetz:\\
\begin{equation*}
\sum\limits_{k=m}^{n}(ca_k+db_k)=c\sum\limits_{k=m}^{n}a_k+d\sum\limits_{k=m}^{n}b_k
\end{equation*}
Indexumbenennung:\\
\begin{equation*}
\sum\limits_{k=m}^{n}a_k=\sum\limits_{i=m}^{n}a_i
\end{equation*}
Indexverschiebung:\\
\begin{equation*}
\sum\limits_{k=m}^{n}a_k=\sum\limits_{i=0}^{n-m}a_{m+i}
\end{equation*}
Multiplikation:\\
\begin{equation*}
\left(\sum\limits_{k=1}^{n}a_k\right)\cdot \left(\sum\limits_{j=1}^{m}b_j\right)=\sum\limits_{k=1}^{n}\sum\limits_{j=1}^{m}a_kb_j=\sum\limits_{j=1}^{m}\sum\limits_{k=1}^{n}a_kb_j
\end{equation*}
Gauß'sche Summenformel:\\
\begin{equation*}
\sum\limits_{k=1}^{n}k=\frac{n(n+1)}{2}
\end{equation*}

\subsection{Produkte}
Arithmetisch wachsende Glieder:\\
\begin{equation*}
\prod\limits_{k=1}^{n}k=n!
\end{equation*}
Geometrisch wachsende Glieder:\\
\begin{equation*}
\prod\limits_{k=0}^{n}q^k=q^{\frac{n(n+1)}{2}}
\end{equation*}

\subsection{Binomialkoeffizient}
\begin{equation*}
(a+b)^n=\sum\limits_{k=0}^{n}\binom{n}{k}a^kb^{n-k}
\end{equation*}
\begin{equation*}
\binom{n}{k}=\frac{n!}{k!(n-k)!}
\end{equation*}
\begin{equation*}
\binom{n}{0}=\binom{n}{n}=1
\end{equation*}
\begin{equation*}
\binom{n+1}{k}=\binom{n}{k}+\binom{n}{k-1}
\end{equation*}

\subsection{Trigonometrie}
\begin{tabular}{|c|c|c|c|c|c|}
\hline $x$ & $0$ & $\frac{\pi}{6}$ & $\frac{\pi}{4}$ & $\frac{\pi}{3}$ & $\frac{\pi}{2}$\\
\hline $\cos(x)$ & $1$ & $\frac{\sqrt{3}}{2}$ & $\frac{\sqrt{2}}{2}$ & $\frac{1}{2}$ & $0$\\
\hline $\sin(x)$ & $0$ & $\frac{1}{2}$ & $\frac{\sqrt{2}}{2}$ & $\frac{\sqrt{3}}{2}$ & $1$\\
\hline $\tan(x)$ & $0$ & $\frac{\sqrt{3}}{3}$ & $1$ & $\sqrt{3}$ & $-$\\
\hline $\cot(x)$ & $-$ & $\frac{3}{\sqrt{3}}$ & $1$ & $\frac{1}{\sqrt{3}}$ & $0$\\
\hline
\end{tabular}\\\\
Sinus und Kosinus mit komplexen Zahlen:
\begin{equation*}
\sin(x)=\frac{e^{ix}-e^{-ix}}{2i}
\end{equation*}
\begin{equation*}
\cos(x)=\frac{e^{ix}+e^{-ix}}{2}
\end{equation*}
Trigonometrischer Pythagoras:\\
\begin{equation*}
\sin^2(x)+\cos^2(x)=1
\end{equation*}
Zum Tangens:\\
\begin{equation*}
\tan(x)=\frac{\sin(x)}{\cos(x)}
\end{equation*}
\begin{equation*}
\cot(x)=\frac{\cos(x)}{\sin(x)}
\end{equation*}
\begin{equation*}
\tan(x+\pi)=\tan(x)
\end{equation*}
\begin{equation*}
\cot(x+\pi)=\cot(x)
\end{equation*}
Additionstheoreme:\\
\begin{equation*}
\cos(x+y)=\cos(x)\cos(y)-\sin(x)\sin(y)
\end{equation*}
\begin{equation*}
\sin(x+y)=\sin(x)\cos(y)+\cos(x)\sin(y)
\end{equation*}
\begin{equation*}
\cos^2(x)=\frac{1+cos(2x)}{2}
\end{equation*}
\begin{equation*}
\sin^2(x)=\frac{1-\cos(2x)}{2}
\end{equation*}
\begin{equation*}
\cos(x)=\frac{1-\tan^2(\frac{x}{2})}{1+\tan^2(\frac{x}{2})}
\end{equation*}
\begin{equation*}
\sin(x)=\frac{2\tan(\frac{x}{2})}{1+\tan^2(\frac{x}{2})}
\end{equation*}

\subsection{Logarithmus}
\begin{equation*}
e^{\ln(x)}=x
\end{equation*}
\begin{equation*}
\ln(e^x)=x
\end{equation*}
\begin{equation*}
a^x=e^{x\ln(a)}
\end{equation*}
\begin{equation*}
b^{\frac{\ln(x)}{\ln(b)}}=x
\end{equation*}
\begin{equation*}
\ln(ab)=\ln(a)+\ln(b)
\end{equation*}
\begin{equation*}
\ln(\frac{a}{b})=ln(a)-\ln(b)
\end{equation*}
\begin{equation*}
\ln(x^a)=a\cdot\ln(x)
\end{equation*}

\subsection{Quadratische Lösungsformel}
Für Polynome zweiten Grades $p(x)=ax^2+bx+c$ gilt für die Lösung der Gleichung $p(x)=0$:
\begin{equation*}
x_{1,2}=\frac{-b\pm\sqrt{b^2-4ac}}{2a}
\end{equation*}

\subsection{Mengen (Begriffe)}
\textbf{Supremum:} Die kleinste obere Schranke der Menge.\\
\textbf{Infimum:} Die größte untere Schranke der Menge.\\
\textbf{Maximum:} = Supremum, falls es in der Menge liegt.\\
\textbf{Minimum:} = Infimum, falls es in der Menge liegt.

\subsection{Komplexe Zahlen}
Schreibweisen:\\
\begin{equation*}
z=x+yi=(x,y)
\end{equation*}
\begin{equation*}
x=Re(z);\;\;\;\;y=Im(z)
\end{equation*}
\begin{equation*}
z=r[\cos(\Phi)+i\sin(\Phi)]=r\cdot e^{i\Phi}=(r,\Phi)
\end{equation*}
\\Umwandlung kartesische Koordinaten $\Leftrightarrow$ Polarkoordinaten:\\
\begin{equation*}
x=r\cdot \cos(\Phi);\;\;\;\;y=r\cdot\sin(\Phi)
\end{equation*}
\begin{equation*}
r=\sqrt{x^2+y^2};\;\;\;\;\Phi=\begin{cases}\arccos(\frac{x}{r}) & y\geq 0 \\ -\arccos(\frac{x}{r}) & y<0\end{cases}
\end{equation*}
Rechenregeln:\\
\begin{equation*}
e^{a+ib}=e^a[\cos(b)+i\sin(b)]
\end{equation*}
\begin{equation*}
z\cdot w =(r_z\cdot r_w, \Phi_z+\Phi_w)
\end{equation*}
\begin{equation*}
z^n=(r^n,n\cdot \Phi)
\end{equation*}
\begin{equation*}
\sqrt{z}=\pm \sqrt{r}\left[\cos\left(\frac{\Phi}{2}\right)+i\sin\left(\frac{\Phi}{2}\right)\right]
\end{equation*}
\begin{equation*}
|z|=\sqrt{x^2+y^2}
\end{equation*}
\begin{equation*}
\overline{z}=x-yi
\end{equation*}
Sonstiges:\\
\begin{equation*}
e^{i\pi}=-1
\end{equation*}
\begin{equation*}
\sqrt{i}=\pm \frac{1+i}{\sqrt{2}}
\end{equation*}

\section{Matrizenrechnung und lineare Gleichungssysteme}

\subsection{Rechenregeln}
Für beliebige Matrizen $A,B,C\in \mathbb{R}^{m\times n}$ und reelle Zahlen $\lambda,\mu\in \mathbb{R}$ gilt:\\
\begin{equation*}
A+B=B+A
\end{equation*}
\begin{equation*}
(A+B)+C=A+(B+C)
\end{equation*}
\begin{equation*}
A+0=A
\end{equation*}
\begin{equation*}
A+(-A)=0
\end{equation*}
\begin{equation*}
(\lambda\mu)A=\lambda(\mu A)
\end{equation*}
\begin{equation*}
1\cdot A=A
\end{equation*}
\begin{equation*}
(\lambda+\mu)A=\lambda A+\mu A
\end{equation*}
\begin{equation*}
\lambda(A+B)=\lambda A+\lambda B
\end{equation*}

\subsection{Gauß'sches Eliminationsverfahren}
\begin{enumerate}
\item Aufstellen der erweiterten Koeffizientenmatrix $(A|b)$
\item Vorwärtselimination mit elementaren Zeilenumformungen mit dem Ziel, die Zeilenstufenform zu erhalten
\item Lösbarkeitstest
\item Falls lösbar, Rückwärtssubstitution
\end{enumerate}

\subsection{Rang einer Matrix}
Die Anzahl der von Null verschiedenen Zeilen von $A$ in der Zeilenstufenform der Matrix nennt man Rang von $A$.

\subsection{Matrizenmultiplikation}
$A\in\mathbb{R}^{m\times n}$ und $B\in\mathbb{R}^{n\times r}\rightarrow C\in\mathbb{R}^{m\times r}$
\begin{equation*}
A=\begin{pmatrix}z_1\\z_2\\...\\z_m\end{pmatrix}
\end{equation*}
\begin{equation*}
B=\begin{pmatrix}s_1 & s_2 & ... & s_r\end{pmatrix}
\end{equation*}
\begin{equation*}
C=AB=\begin{pmatrix}z_1s_1 & z_1s_2 & ... & z_1s_r \\ z_2s_1 & z_2s_2 & ... & z_2s_r \\ ... & ... & & ... \\ z_ms_1 & z_ms_2 & ... & z_ms_r\end{pmatrix}
\end{equation*}
Rechenregeln:
\begin{equation*}
(A+B)C=AC+BC
\end{equation*}
\begin{equation*}
\alpha(AB)=(\alpha A)B = A(\alpha B)
\end{equation*}
\begin{equation*}
A(BC)=(AB)C
\end{equation*}
Im Allgemeinen gilt:
\begin{equation*}
AB \neq BA
\end{equation*}

\subsection{Transponierte einer Matrix}
Vertauschen der Spalten und Zeilen ($\mathbb{R}^{m\times n}\rightarrow \mathbb{R}^{n\times m}$).\\\\
Rechenregeln
\begin{equation*}
(A+B)^T=A^T+B^T
\end{equation*}
\begin{equation*}
(\alpha A)^T=\alpha A^T
\end{equation*}
\begin{equation*}
(A^T)^T =A
\end{equation*}
\begin{equation*}
(AB)^T=B^TA^T
\end{equation*}
Für Spaltenvektoren $a,b$ gilt:
\begin{equation*}
a^Tb=b^Ta
\end{equation*}

\subsection{Invertierbare Matrizen (= regulär bzw. nicht singulär)}

\subsubsection{Allgemein}
Für eine quadratische Matrix $A\in\mathbb{R}^{n\times n}$ sind folgende Aussagen äquivalent:
\begin{enumerate}
\item $A$ ist invertierbar
\item $Rang(A)=n$
\item $det(A)\neq 0$
\item Kern(A)={0}
\item Die Spalten von $A$ sind linear unabhängig
\item Die Zeilen von $A$ sind linear unabhängig
\item $dim(S_A)=dim(Z_A)=n$
\item $0$ ist kein Eigenwert von $A$

\end{enumerate}

\subsubsection{Rechenregeln}
\begin{equation*}
(A^{-1})^{-1}=A
\end{equation*}
\begin{equation*}
(AB)^{-1}=B^{-1}A^{-1}
\end{equation*}
\begin{equation*}
(A^T)^{-1}=(A^{-1})^T
\end{equation*}

\subsubsection{Gauß-Jordan-Verfahren}
\begin{enumerate}
\item Man ''erweitert'' die Matrix $A$ mit der Einheitsmatrix und betrachtet die erweiterte Matrix $(A|I_n)$
\item Durch Vorwärtselimination bringt man die Matrix auf die Form $(M|N)$, wobei $M$ die Zeilenstufenform der Matrix $A$ ist
\item Hat die Matrix $M$ keine Nullzeile, so ist die Matrix $A$ invertierbar
\item Durch Rückwärtssubstitution formt man die Matrix $M$ so um, dass daraus die Einheitsmatrix entsteht. Dann erhält man aus der Matrix $N$ die Inverse $(I_n|A^{-1})$
\end{enumerate}

\subsection{Spezielle Matrizen}

\subsubsection{Symmetrische Matrizen}
Es gilt:
\begin{equation*}
A=A^T
\end{equation*}

\subsubsection{Schiefsymmetrische Matrizen}
Es gilt:
\begin{equation*}
A=-A^T
\end{equation*}

\subsubsection{Orthogonale Matrizen}
Eine Matrix $A\in\mathbb{R}^{n\times n}$ heißt orthogonal, wenn eine der folgenden äquivalenten Eigenschaften erfüllt ist:
\begin{equation*}
A^TA=AA^T=I_n
\end{equation*}
\begin{equation*}
A^T=A^{-1}
\end{equation*}
\begin{equation*}
||Ax||_2 = ||x||_2
\end{equation*}
\begin{equation*}
(Ax, Ay)=(x,y)
\end{equation*}
Eigenschaften orthogonaler Matrizen:
\begin{equation*}
det(A)=\pm 1
\end{equation*}
\begin{center}
Spalten von $A$ bilden Orthonormalbasis
\end{center}
\begin{equation*}
|\lambda|=1\;\;\;(Eigenwert)
\end{equation*}

\section{Determinanten}

\subsection{Rechenregeln}
\begin{equation*}
det(a_{11})=a_{11}
\end{equation*}
\begin{equation*}
det\begin{pmatrix}a_{11} & a_{12} \\ a_{21} & a_{22}\end{pmatrix}=a_{11}a_{22}-a_{12}a_{21}
\end{equation*}
Entwicklung nach der $j$-ten Spalte:
\begin{equation*}
det(A)=\sum\limits_{i=1}^{n}(-1)^{i+j}a_{ij}det(A_{ij})
\end{equation*}
Entwicklung nach der $i$-ten Zeile:
\begin{equation*}
det(A)=\sum\limits_{j=1}^{n}(-1)^{i+j}a_{ij}det(A_{ij})
\end{equation*}
Für untere und obere Dreiecksmatrix gilt:
\begin{equation*}
det(A)=\prod\limits_{i=1}^{n}a_{ii}
\end{equation*}
Weitere Rechenregeln:
\begin{equation*}
det\begin{pmatrix}z_1 \\ ... \\ \alpha z_i \\ ... \\ z_n\end{pmatrix}=\alpha det\begin{pmatrix}z_1 \\ ... \\ z_i \\ ... \\ z_n\end{pmatrix}
\end{equation*}
\begin{equation*}
det\begin{pmatrix}z_1 \\ ... \\ a+b \\ ... \\ z_n\end{pmatrix}=det\begin{pmatrix}z_1 \\ ... \\ a \\ ... \\ z_n\end{pmatrix}+det\begin{pmatrix}z_1 \\ ... \\ b \\ ... \\ z_n\end{pmatrix}
\end{equation*}
\begin{equation*}
det(\alpha A)=\alpha^n det(A)
\end{equation*}
Im Allgemeinen gilt:
\begin{equation*}
det(A+B)\neq det(A)+det(B)
\end{equation*}
Vertauscht man zwei Zeilen einer Matrix $A$ und erhält $B$, so gilt:
\begin{equation*}
det(B)=-det(A)
\end{equation*}
Multipliziert man eine Zeile einer Matrix $A$ mit einem Faktor $\alpha$ und erhält $B$, so gilt:
\begin{equation*}
det(B)=\alpha det(A)
\end{equation*}
Addiert man das Vielfache einer Zeile zu einer anderen Zeile einer Matrix $A$ und erhält $B$, so gilt:
\begin{equation*}
det(B)=det(A)
\end{equation*}
Seien $A,B,C\in\mathbb{R}^{n\times n}$, dann gilt
\begin{equation*}
det(AB)=det(A)det(B)
\end{equation*}
\begin{equation*}
det(A^T)=det(A)
\end{equation*}
Falls $A$ invertierbar:
\begin{equation*}
det(A^{-1})=(det(A))^{-1}
\end{equation*}
Für jede invertierbare Matrix $C$ gilt:
\begin{equation*}
det(C^{-1}AC)=det(A)
\end{equation*}

\subsection{Cramer'sche Regel}
Sei $A\in \mathbb{R}^{n\times n}$ eine invertierbare Matrix, dann kann man die Lösung des Gleichungssystems $Ax=b$ wie folgt angeben:
\begin{equation*}
x_i=\frac{1}{det(A)}det\begin{pmatrix}s_1 & ... & s_{i-1} & b & s_{i+1} & ... & s_n\end{pmatrix}
\end{equation*}

\section{Vektorräume}

\subsection{Bedingungen für einen Vektorraum}
Eine nichtleere Menge $V$, in der man zu je zwei Elementen $a,b\in V$ eine Summe $a+b\in V$ und zu jedem Element $a\in V$ und zu jeder reellen Zahl $\lambda \in\mathbb{R}$ das $\lambda$-fache $\lambda a\in V$ bilden kann, heißt Vektorraum, wenn folgende acht Rechengesetze erfüllt sind:
\begin{enumerate}
\item Die Addition ist kommutativ:
\begin{equation*}
a+b=b+a
\end{equation*}
\item Die Addition ist assoziativ:
\begin{equation*}
(a+b)+c=a+(b+c)
\end{equation*}
\item Es gibt ein Element $0\in V$ (Nullelement, Nullvektor) mit
\begin{equation*}
a+0=a
\end{equation*}
\item Zu jedem Element $a\in V$ gibt es genau ein mit $-a$ bezeichnetes Element in $V$ mit
\begin{equation*}
a+(-a)=0
\end{equation*}
\item Es gilt:
\begin{equation*}
1a=a
\end{equation*}
\item Es gilt:
\begin{equation*}
\lambda(\mu a)=(\lambda\mu)a
\end{equation*}
\item Es gilt:
\begin{equation*}
\lambda(a+b)=\lambda a+\lambda b
\end{equation*}
\item Es gilt:
\begin{equation*}
(\lambda+\mu)a=\lambda a+\mu a
\end{equation*}
\end{enumerate}

\subsection{Unterräume}
Eine nichtleere Teilmenge $W\subset V$ eines Vektorraums $V$ heißt Unterraum von $V$, wenn mit je zwei Elementen $x,y\in W$ auch die Summe $x+y$ in $W$ liegt und mit jedem Element $x$ auch das $\lambda$-fache $\lambda x$ in $W$ liegt ($\rightarrow$ es muss somit auch gelten: $0\in W$)

\subsection{Kern einer Matrix}
Der Kern einer $(m\times n)$-Matrix $A$ ist die Lösungsmenge des homogenen linearen Gleichungssystems $Ax=0$
\begin{equation*}
Kern(A)=\{x\in\mathbb{R}^n|Ax=0\}
\end{equation*}
$Kern(A)$ ist immer ein Untervektorraum von $V=\mathbb{R}^n$

\subsection{Lineare Hülle}
Ein Unterraum $U$ von $V$ wird von den Vektoren $v_1,v_2,...,v_k$ erzeugt (aufgespannt), wenn gilt:
\begin{equation*}
U=Lin(v_1,v_2,...,v_k)
\end{equation*}
Dann heißt $\{v_1,v_2,...,v_k\}$ ein Erzeugendensystem von $U$.

\subsection{Lineare Abhängigkeit}
Die Vektoren $v_1,v_2,...,v_k$ in einem Vektorraum $V$ heißen linear abhängig, wenn es Zahlen $\alpha_1, \alpha_2,...,\alpha_k \in\mathbb{R}$ gibt, die nicht alle gleich Null sind, sodass gilt:
\begin{equation*}
\alpha_1v_1+\alpha_2v_2+...+\alpha_kv_k=0
\end{equation*}
Die Vektoren heißen linear unabhängig, wenn sich nur die triviale Linearkombination bilden lässt.

\subsubsection{Eigenschaften}
\begin{enumerate}
\item Jedes endliche System von Vektoren, das den Nullvektor enthält, ist linear abhängig
\item Jedes endliche System von Vektoren, das ein Teilsystem linear abhängiger Vektoren enthält, ist linear abhängig
\item Jedes Teilsystem eine Systems linear unabhängiger Vektoren ist linear unabhängig
\end{enumerate}

\subsection{Basis}
Ein System $(v_1,v_2,...,v_k)$ heißt Basis des Vektorraums $V$, wenn gilt:
\begin{enumerate}
\item Die Vektoren $v_1,v_2,...,v_k$ sind linear unabhängig
\item Die Vektoren $v_1,v_2,...,v_k$ erzeugen $V$ (lineare Hülle)
\end{enumerate}

\subsection{Dimension}
Die Länge einer Basis eines endlich erzeugten Vektorraumes $V\neq \{0\}$ nennt man Dimension und bezeichnet man mit $dim(V)$.\\
Für den Fall $V=\{0\}$ setzt man $dim(V)=0$.\\\\
Dimensionsformel:\\
Sei $A\in\mathbb{R}^{m\times n}$, dann gilt:
\begin{equation*}
Rang(A)+dim(Kern(A))=n
\end{equation*}

\subsection{Zeilen- und Spaltenräume einer Matrix}
Sei $A\in\mathbb{R}^{m\times n}$ und $B\in\mathbb{R}^{n\times k}$

\subsubsection{Spaltenraum}
Der Spaltenraum $S_A\subset \mathbb{R}^m$ ist die lineare Hülle der Spaltenvektoren:
\begin{equation*}
S_A=Lin(s_1,s_2,...,s_n)
\end{equation*}
\begin{equation*}
S_A=\{Ax|x\in\mathbb{R}^n\}
\end{equation*}

\subsubsection{Zeilenraum}
Der Zeilenraum ist die lineare Hülle der Zeilenvektoren
\begin{equation*}
Z_A=Lin(z_1,z_2,...,z_m)
\end{equation*}
\begin{equation*}
Z_A=\{y^TA|y\in\mathbb{R}^m\}
\end{equation*}

\subsubsection{Eigenschaften}
\begin{equation*}
Rang(A)=Rang(A^T)=dim(S_A)=dim(Z_A)
\end{equation*}
Zeilenräume ändern sich nicht bei elementaren Zeilentransformationen. Analoges gilt für Spaltenräume.\\\\
Des Weiteren gilt:
\begin{equation*}
Rang(A)\geq Rang(AB)\leq Rang(B)
\end{equation*}

\section{Metrik im euklidischen Vektorraum}

\subsection{Skalarprodukt}
Sei $V$ ein Vektorraum. Eine Abbildung, die je zwei Vektoren $x,y\in V$ eine Zahl $(x,y)\in\mathbb{R}$ zuordnet, heißt Skalarprodukt, wenn folgende Eigenschaften erfüllt sind:\\\\
1. Symmetrie
\begin{equation*}
(x,y)=(y,x)
\end{equation*}
2. Linearität in jedem Faktor
\begin{equation*}
(x,\alpha y+\beta z)=\alpha (x,y)+\beta (x,z)
\end{equation*}
3. Positive Definitheit
\begin{equation*}
(x,x)>0\;mit\;x\neq 0
\end{equation*}

\subsubsection{Skalarprodukt auf Matrizen}
Eine Matrix $A\in\mathbb{R}^{n\times n}$ heißt positiv definit, wenn gilt:
\begin{equation*}
x^TAx>0\;mit\;x\neq 0
\end{equation*}
Skalarprodukt mit symmetrischen und positiv definiten Matrizen:
\begin{equation*}
(x,y)_A=x^TAy
\end{equation*}
Positivitätstest:\\
Eine symmetrische Matrix $A\in\mathbb{R}^{n\times n}$ ist genau dann positiv definit, wenn die Determinanten der $n$ sogenannten Hauptuntermatrizen $H_i$ positiv sind:
\begin{equation*}
H_1=(a_{11})
\end{equation*}
\begin{equation*}
H_2=\begin{pmatrix}a_{11} & a_{12} \\ a_{21} & a_{22}\end{pmatrix}
\end{equation*}

\subsection{Norm}
\begin{equation*}
||x||=\sqrt{(x,x)}
\end{equation*}

\subsubsection{Regeln für Normen}
1. Definitheit
\begin{equation*}
||x||\geq 0\;\forall\;x\in V\;und\;||x||=0\;falls\;x=0
\end{equation*}
2. Homogenität
\begin{equation*}
||\alpha x|| = |\alpha|\cdot ||x||
\end{equation*}
3. Dreiecksungleichung
\begin{equation*}
||x+y||\leq ||x||+||y||
\end{equation*}

\subsubsection{$l_p$-Norm und $l_\infty$-Norm}
\begin{equation*}
||x||_p=(|x_1|^p+|x_2|^p+...+|x_n|^p)^{\frac{1}{p}}
\end{equation*}
\begin{equation*}
||x||_\infty =\max\limits_{1\leq i\leq n}|x_i|
\end{equation*}

\subsubsection{Cauchy-Schwarz-Ungleichung}
Sei $(.,.)$ ein Skalarprodukt auf dem Vektorraum $V$ und $||.||$ die von dem Skalarprodukt induzierte Norm. Dann gilt:
\begin{equation*}
|(x,y)|\leq ||x||\cdot ||y||
\end{equation*}

\subsection{Winkel}
Der Winkel $\Phi = \angle (x,y)$ zwischen zwei Vektoren $x,y\in V$ mit $x\neq 0,y\neq 0$ wird definiert als
\begin{equation*}
0\leq\Phi\leq\pi\;mit\;\cos(\Phi)=\frac{(x,y)}{||x||\cdot ||y||}
\end{equation*}

\subsection{Orthogonale Zerlegung}
Sei $V$ ein euklidischer Vektorraum. Zwei Vektoren $x,y\in V$ heißen orthogonal ($x\bot y$), wenn $(x,y)=0$ gilt.\\
Ein System von Vektoren $\{v_1,v_2,...v_k\}$ heißt orthogonal, falls die Vektoren $v_i$ paarweise orthogonal sind.\\
Sind die Vektoren $\{v_1,v_2,...v_k\}$ paarweise orthogonal und alle ungleich Null, dann sind sie linear unabhängig.\\
Ein System von Vektoren heißt Orthogonalbasis eines linearen Raumes $V$, wenn es eine Basis darstellt und orthogonal ist.\\
Eine Basis heißt orthonormal, wenn sie orthogonal ist und die Basisvektoren die Länge 1 haben, d.h.
\begin{equation*}
(v_i,v_j)=\delta_{ij}=\begin{cases}1 & falls\;i=j \\ 0 & falls\;i\neq j\end{cases}
\end{equation*}
Sei $U$ ein Unterraum eines euklidischen Vektorraumes $V$ mit $dim(V)=n$. Man definiert den zu $U$ orthogonalen Raum $U^{\bot}$ durch
\begin{equation*}
U^{\bot}=\{v\in V|(v,u)=0\;\forall\;u\in U\}
\end{equation*}
Es gilt:
\begin{equation*}
U\cap U^{\bot}=\{0\}
\end{equation*}
\begin{equation*}
dim(U^{\bot})=n-dim(U)
\end{equation*}
Sei $x\in V=\mathbb{R}^n$ gegeben. Die orthogonale Zerlegung von $x$ bezüglich eines Unterraumes $U\subset V$ ist eine Darstellung
\begin{equation*}
x=x_U+x_{U^{\bot}}
\end{equation*}
mit
\begin{equation*}
x_U\in U\;und\;x_{U^{\bot}}\in U^{\bot}
\end{equation*}
Der Vektor $x_U$ ist dann die orthogonale Projektion von $x$ auf $U$\\\\
Sei $v_1\in U$ und $U$ 1-dimensional. Dann gilt:
\begin{equation*}
x=\lambda v_1+x_{U^{\bot}}
\end{equation*}
und somit
\begin{equation*}
(x,v_1)=\lambda(v_1,v_1)+\underbrace{(x_{U^{\bot}},v_1)}_{=0}
\end{equation*}
Sei $v_i\in U$ und eine Orthonormalbasis. Dann gilt:
\begin{equation*}
x_U=(x,v_1)v_1+(x,v_2)v_2+...+(x,v_n)v_n
\end{equation*}

\subsubsection{Gram-Schmidt-Orthogonalisierung}
Sei eine Basis $(v_1,v_2,...,v_r)$ eines (Unter-)Vektorraumes $W$ gegeben:
\begin{enumerate}
\item Man normiert den ersten Vektor
\begin{equation*}
b_1=\frac{1}{||v_1||}v_1
\end{equation*}
\item Man bestimmt die zu $b_1$ orthogonale Komponente von $v_2$
\begin{equation*}
z_2=v_2-(v_2,b_1)b_1
\end{equation*}
die anschließend normiert wird
\begin{equation*}
b_2=\frac{1}{||z_2||}z_2
\end{equation*}
\item Analog wird dann die zu $b_1$ und $b_2$ orthogonale Komponente von $v_3$ bestimmt
\begin{equation*}
z_3=v_3-(v_3,b_2)b_2-(v_3,b_1)b_1
\end{equation*}
und normiert
\begin{equation*}
b_3=\frac{1}{||z_3||}z_3
\end{equation*}
\item usw...
\end{enumerate}

\subsection{Vektorprodukt (Kreuzprodukt)}
Seien $a,b\in\mathbb{R}^3$ Spaltenvektoren mit
\begin{equation*}
a=\begin{pmatrix}a_1 \\ a_2 \\ a_3\end{pmatrix}\;und\;b=\begin{pmatrix}b_1 \\ b_2 \\ b_3\end{pmatrix}
\end{equation*}
dann ist das Vektorprodukt $c=a\times b$ gegeben als
\begin{equation*}
c=\begin{pmatrix}a_2b_3-a_3b_2 \\ a_3b_1-a_1b_3 \\ a_1b_2-a_2b_1\end{pmatrix}
\end{equation*}

\subsubsection{Eigenschaften}
\begin{enumerate}
\item $a\times b$ steht rechtwinklig auf $a$ und $b$
\item Die Länge des Vektors $a\times b$ ist gleich der Fläche des Parallelogramms mit den Seiten $a$ und $b$
\item Der Winkel $\Phi$ zwischen $a$ und $b$ ist definiert durch
\begin{equation*}
\sin(\Phi)=\frac{||a\times b||}{||a||\cdot ||b||}
\end{equation*}
\item $a\times a=0$
\item $a\times b=-b\times a$
\item $a\times b=0$, wenn $a$ und $b$ linear abhängig sind
\item $||a\times b||^2 +|(a,b)|^2=||a||^2 +||b||^2 $
\end{enumerate}

\subsection{Spatprodukt}
Seien die Vektoren $a,b,c\in \mathbb{R}^3$ gegeben. Das Spatprodukt dieser Vektoren ist definiert als
\begin{equation*}
[a,b,c]=(a\times b,c)=det\begin{pmatrix}a_1 & b_1 & c_1 \\ a_2 & b_2 & c_2 \\ a_3 & b_3 & c_3\end{pmatrix}
\end{equation*}

\subsubsection{Eigenschaften}
\begin{enumerate}
\item Der Betrag des Spatprodukts $[a,b,c]$ ist gleich dem Volumen des von $a,b$ und $c$ aufgespannten Spats.
\item Das Spatprodukt ist positiv, wenn $a,b$ und $c$ ein Rechtssystem bilden
\item $[a,b,c]=[b,c,a]=[c,a,b]$ (zyklische Vertauschung)
\item $[a,b,c]=-[a,c,b]$ (allgemeine Vertauschung)
\item $[a,a,b]=0$
\end{enumerate}

\section{Folgen und Reihen}

\subsection{Folgen}
Eine Folge $(a_n)$ konvergiert gegen eine Zahl $a\in\mathbb{R}$
\begin{equation*}
\lim\limits_{n\rightarrow\infty}a_n=a\;oder\;a_n\rightarrow a,\;n\rightarrow\infty
\end{equation*}
falls es für jede positive Zahl $\epsilon>0$ einen Index $N_{\epsilon}\in \mathbb{N}$ gibt, sodass gilt
\begin{equation*}
|a_n-a|<\epsilon\;\forall\;n>N_{\epsilon}
\end{equation*}

\subsubsection{Rechenregeln für Folgen und Grenzwerte}
Rechenregeln für konvergente Folgen $(a_n)$ und $(b_n)$ und ihre Grenzwerte $a$ und $b$:
\begin{equation*}
\lim\limits_{n\rightarrow\infty}(a_n+b_n)=a+b
\end{equation*}
\begin{equation*}
\lim\limits_{n\rightarrow\infty}(a_nb_n)=ab
\end{equation*}
\begin{equation*}
\lim\limits_{n\rightarrow\infty}(\frac{a_n}{b_n})=\frac{a}{b}\;falls\;b\neq 0
\end{equation*}
\begin{equation*}
\lim\limits_{n\rightarrow\infty}\sqrt{a_n}=\sqrt{a}\;falls\; a_n\geq 0
\end{equation*}
\begin{equation*}
\lim\limits_{n\rightarrow\infty}|a_n|=|a|
\end{equation*}
\begin{equation*}
a_n\leq b_n\Rightarrow a\leq b
\end{equation*}

\subsubsection{Grenzwertbestimmung und Konvergenzbeweis}
\textbf{Nachweis von Monotonie und Beschränktheit}\\
Eine Folge $(a_n)$ konvergiert gegen eine Zahl $a\in\mathbb{R}$ falls sie beschränkt und dementsprechend monoton ist.\\\\
\textbf{Sandwich-Theorem}\\
Seien $(a_n)$ und $(b_n)$ zwei konvergente Folgen mit demselben Grenzwert $b$ und erfüllt die Folge $(c_n)$ die Ungleichung
\begin{equation*}
a_n\leq c_n\leq b_n\;\forall\;n>N,\;N\in \mathbb{N}
\end{equation*}
so ist die Folge $(c_n)$ ebenfalls konvergent und es gilt
\begin{equation*}
\lim\limits_{n\rightarrow\infty}c_n=b
\end{equation*}
\textbf{Cauchy-Konvergenzkriterium}\\
Eine Folge $(a_n)$ ist genau dann konvergent, wenn für alle hinreichend großen Indizes $m$ und $n$ die Abstände $|a_m-a_n|$ beliebig klein werden.
Das heißt, wenn es zu jeder Zahl $\epsilon>0$ einen Index $N_{\epsilon}$ gibt, sodass gilt:
\begin{equation*}
|a_m-a_n|<\epsilon\;\forall\;m,n>N_{\epsilon}
\end{equation*}
\textbf{Grenzwert-Berechnung}\\
Ist die Konvergenz bewiesen, erhält man den Grenzwert durch Gleichsetzen von $a_{n+1}$ und $a_n$.

\subsubsection{Wichtige Folgen und Grenzwerte}
Geometrische Folge:
\begin{equation*}
\lim\limits_{n\rightarrow\infty}q^n=\begin{cases}0 & falls\;|q|<1 \\ 1 & falls\; q=1 \\ +\infty & falls\;q>1 \\ divergent & falls\;q\leq -1\end{cases}
\end{equation*}
Eulersche Zahl:
\begin{equation*}
\lim\limits_{n\rightarrow\infty}\left(1+\frac{x}{n}\right)^n=e^x
\end{equation*}
Andere Grenzwerte:
\begin{equation*}
\lim\limits_{n\rightarrow\infty}\frac{n^r}{q^n}=0\;falls\;q>1,\;r\in\mathbb{N}
\end{equation*}
\begin{equation*}
\lim\limits_{n\rightarrow\infty}\frac{2^n}{n!}=0
\end{equation*}
\begin{equation*}
\lim\limits_{n\rightarrow\infty}\frac{n}{2^n}=0
\end{equation*}
\begin{equation*}
\lim\limits_{n\rightarrow\infty}\sqrt[n]{n}=1
\end{equation*}
\begin{equation*}
\lim\limits_{n\rightarrow\infty}\frac{\ln^k(n)}{n^{\epsilon}}=0\;falls\; k\in\mathbb{N},\epsilon>0
\end{equation*}

\subsection{Unendliche Reihen}

\subsubsection{Rechenregeln für Reihen}
Für zwei konvergent Reihen $\sum\limits_{n=1}^{\infty}a_n$ und $\sum\limits_{n=1}^{\infty}b_n$ gilt:
\begin{equation*}
\sum\limits_{n=1}^{\infty}(a_n\pm b_n)=a\pm b
\end{equation*}
\begin{equation*}
\sum\limits_{n=1}^{\infty}ca_n=c\sum\limits_{n=1}^{\infty}a_n\;\forall\;c\in\mathbb{R}
\end{equation*}

\subsubsection{Grenzwertbestimmung und Konvergenzkriterien}
Ist $\sum\limits_{n=1}^{\infty}a_n$ eine konvergente Reihe, so ist $a_n$ eine Nullfolge.\\
Eine Reihe $\sum\limits_{n=1}^{\infty}a_n$ heißt absolut konvergent, wenn $\sum\limits_{n=1}^{\infty}|a_n|$ konvergent ist.\\
Jede absolut konvergente Reihe ist konvergent (aber nicht umgekehrt!).\\\\
\textbf{Leibnizkriterium}\\
Sei $(a_n)$ eine monoton fallende Nullfolge (mit $a_n>0$), dann konvergiert die alternierende Reihe
\begin{equation*}
\sum\limits_{n=1}^{\infty}(-1)^na_n
\end{equation*}
\textbf{Majorantenkriterium}\\
Seien $(a_n)$ und $(b_n)$ zwei Folgen, für die die Bedingung
\begin{equation*}
|a_n|\leq b_n\;\forall\; n\geq N
\end{equation*}
mit einem Index $N\in\mathbb{N}$ erfüllt ist. Dann gelten die folgenden beiden Aussagen:
\begin{enumerate}
\item Ist die Reihe $\sum\limits_{n=1}^{\infty}b_n$ konvergent, so ist die Reihe $\sum\limits_{n=1}^{\infty}a_n$ absolut konvergent
\item Ist die Reihe $\sum\limits_{n=1}^{\infty}|a_n|$ divergent, so ist auch die Reihe $\sum\limits_{n=1}^{\infty}b_n$ divergent.
\end{enumerate}
\textbf{Quotientenkriterium}\\
Für die Folge $(a_n)$ betrachtet man die Folge des Quotienten $\frac{a_{n+1}}{a_n}$
\begin{enumerate}
\item Es gelte
\begin{equation*}
\left|\frac{a_{n+1}}{a_n}\right|\leq q<1\;\forall\;n>N
\end{equation*}
mit einem Index $N\in\mathbb{N}$ und einer Zahl $0<q<1$. Dann ist die Reihe $\sum\limits_{n=1}^{\infty}a_n$ absolut konvergent.
\item Es gelte
\begin{equation*}
\left|\frac{a_{n+1}}{a_n}\right|\geq 1\;\forall\;n>N
\end{equation*}
mit einem Index $N\in\mathbb{N}$, so ist die Reihe $\sum\limits_{n=1}^{\infty}a_n$ divergent.
\end{enumerate}
\textbf{Wurzelkriterium}\\
Sei eine Reihe $\sum\limits_{n=1}^{\infty}a_n$ gegeben. Existiert der Grenzwert
\begin{equation*}
\mu =\lim\limits_{n\rightarrow\infty}\sqrt[n]{|a_n|}
\end{equation*}
so gilt:
\begin{enumerate}
\item Ist $\mu<1$, so konvergiert die Reihe absolut
\item Ist $\mu>1$, so divergiert die Reihe
\item Ist $\mu=1$, so kann man mit dem Wurzelkriterium keine Aussage über die Konvergenz treffen.
\end{enumerate}

\subsubsection{Wichtige Reihen und Grenzwerte}
Geometrische Reihe
\begin{equation*}
\sum\limits_{n=0}^{\infty}q^n=\frac{1}{1-q}\;falls\;|q|<1
\end{equation*}
\begin{equation*}
\sum\limits_{i=0}^{n}q^i=\frac{q^{n+1}-1}{q-1}\;falls\;q\neq 1
\end{equation*}
Harmonische Reihe
\begin{equation*}
\sum\limits_{n=1}^{\infty}\frac{1}{n^{\alpha}}=\begin{cases}divergiert & falls\;\alpha \leq 1 \\ konvergiert & falls\; \alpha >1\end{cases}
\end{equation*}
Exponentialfunktion
\begin{equation*}
\sum\limits_{n=1}^{\infty}\frac{n\cdot\alpha^n}{n!}=\sum\limits_{n=0}^{\infty}\frac{\alpha^n}{n!}=e^{\alpha}
\end{equation*}
$\alpha$ kann hierbei auch komplex sein!\\\\
Logarithmus naturalis
\begin{equation*}
\ln(1+x)=\sum\limits_{k=0}^{\infty}\frac{(-1)^k}{k+1}x^{k+1},\;x\in (-1,1]
\end{equation*}
Trigonometrische Funktionen
\begin{equation*}
\sin(x)=\sum\limits_{k=0}^{\infty}(-1)^k\frac{x^{2k+1}}{(2k+1)!}=x-\frac{x^3}{3!}+\frac{x^5}{5!}-...
\end{equation*}
\begin{equation*}
\cos(x)=\sum\limits_{k=0}^{\infty}(-1)^k\frac{x^{2k}}{(2k)!}=1-\frac{x^2}{2!}+\frac{x^4}{4!}-...
\end{equation*}

\section{Funktionsgrenzwerte und Stetigkeit}

\subsection{Funktionsgrenzwerte}
Der Grenzwert existiert, wenn linksseitiger und rechtsseitiger Grenzwert existieren und übereinstimmen
\begin{equation*}
\lim\limits_{x\rightarrow a}f(x)=\lim\limits_{x\rightarrow a^+}f(x)=\lim\limits_{x\rightarrow a^-}f(x)=c
\end{equation*}
Eine reelle Zahl $c$ heiß Grenzwert der Funktion $f:I\rightarrow\mathbb{R}$ für $x$ gegen $a\in I$, wenn es für jede positive reelle Zahl $\epsilon>0$ eine positive Zahl $\delta>0$ gibt, sodass für alle $x\in I, x\neq a$ mit
\begin{equation*}
|x-a|<\delta
\end{equation*}
die Funktionswerte in der $\epsilon$-Umgebung von $c$ liegen, d.h.
\begin{equation*}
|f(x)-c|<\epsilon
\end{equation*}

\subsubsection{Rechenregeln}
Mit $\lim\limits_{x\rightarrow a}f(x)=c$ und $\lim\limits_{x\rightarrow a}g(x)=d$ mit $c,d\in\mathbb{R}$ folgt:
\begin{equation*}
\lim\limits_{x\rightarrow a}(f(x)+g(x))=c+d
\end{equation*}
\begin{equation*}
\lim\limits_{x\rightarrow a}(f(x)g(x))=cd
\end{equation*}
\begin{equation*}
\lim\limits_{x\rightarrow a}\frac{f(x)}{g(x)}=\frac{c}{d}\;falls\;d\neq 0
\end{equation*}
\begin{equation*}
\lim\limits_{x\rightarrow a}\alpha f(x)=\alpha c\;mit\; \alpha\in\mathbb{R}
\end{equation*}

\subsubsection{Konvergenzbeweise}
\textbf{Vergleichskriterium}\\
Seien $f,g,h:I\rightarrow\mathbb{R}$ drei Funktionen und gelte in der $\delta$-Umgebung von $\alpha\in\mathbb{R}$
\begin{equation*}
g(x)\leq f(x)\leq h(x)\;\forall\;x\in(a-\delta , a+\delta),\delta >0
\end{equation*}
dann folgt aus der Bedingung
\begin{equation*}
\lim\limits_{x\rightarrow a}g(x)=\lim\limits_{x\rightarrow a}h(x)=c
\end{equation*}
die Konvergenz der Funktion $f$
\begin{equation*}
\lim\limits_{x\rightarrow a}f(x)=c
\end{equation*}

\subsubsection{Wichtige Grenzwerte}
\begin{equation*}
\lim\limits_{x\rightarrow 0}\frac{\sin(x)}{x}=1
\end{equation*}
\begin{equation*}
\lim\limits_{x\rightarrow 0}\frac{e^x-1}{x}=1
\end{equation*}
\begin{equation*}
\lim\limits_{x\rightarrow 0}x\cdot \sin(\frac{1}{x})=0
\end{equation*}
\begin{equation*}
\lim\limits_{x\rightarrow \infty} \frac{n!}{n^n}=0
\end{equation*}
Variablentransformation:
\begin{equation*}
\lim\limits_{x\rightarrow a}\frac{e^{x-a}-1}{x-a}=\lim\limits_{y\rightarrow 0}\frac{e^y-1}{y}
\end{equation*}

\subsection{Stetige Funktionen}
Sei $I\subset\mathbb{R}$ ein Intervall, sei $f:I\rightarrow\mathbb{R}$ und $x_0\in I$. Die Funktion $f$ heißt stetig im Punkt $x_0$, falls folgende Bedingung erfüllt ist:
\begin{equation*}
\lim\limits_{x\rightarrow x_0}f(x)=f(x_0)
\end{equation*}
Sei $I=[a,b]$ ein beschränktes und abgeschlossenes Intervall und $f:I\rightarrow\mathbb{R}$ eine stetige Funktion, dann gelten folgende Aussagen:
\begin{enumerate}
\item Die Funktion $f$ ist auf $I$ beschränkt. Das heißt, es gibt eine Konstante $C$ mit
\begin{equation*}
|f(x)|\leq C\;\forall\;x\in I
\end{equation*}
\item Die Funktion $f$ nimmt ihr Minimum und ihr Maximum in $I$ an. Das heißt, es gibt Zahlen $x_{min},x_{max}\in I$ mit
\begin{equation*}
m=\min\limits_{x\in I}f(x)=f(x_{min})\;und\;M=\max\limits_{x\in I}f(x)=f(x_{max})
\end{equation*}
\item \textbf{Zwischenwertsatz}\\
Jeder Zwischenwert $c$ mit $m\leq c\leq M$ wird angenommen.\\
Das heißt, für jedes $c$ mit $m\leq c\leq M$ gibt es ein $\xi\in I$, sodass gilt:
\begin{equation*}
c=f(\xi)
\end{equation*}
\item Haben $f(a)$ und $f(b)$ verschiedene Vorzeichen, so gibt es mindestens eine Nullstelle zwischen $a$ und $b$
\end{enumerate}

\subsubsection{Bisektionsverfahren}
Sei $f:[a,b]\rightarrow\mathbb{R}$ eine stetige Funktion und es gelte $f(a)f(b)<0$, dann existiert mindestens eine Nullstelle in $[a,b]$, die folgendermaßen bestimmt werden kann:
\begin{enumerate}
\item Setze $a_0=a$ und $b_0=b$
\item Für jedes $n\in\mathbb{N}$ berechnet man $c=\frac{a_n+b_n}{2}$.\\
Ist $f(c_n)=0$, so hat man eine Nullstelle gefunden. Andernfalls setzt man\\
$\begin{cases}
a_{n+1}=a_n;b_{n+1}=c_n & falls\;f(a_n)f(c_n)<0 \\ a_{n+1}=c_n; b_{n+1}=b_n & falls\;f(c_n)f(b_n)<0
\end{cases}
$\\
und wiederholt das Ganze bis zur gewünschten Genauigkeit
\end{enumerate}

\subsubsection{Rechenregeln zur Stetigkeit}
Seine $f,g$ zwei stetige Funktionen, dann sind auch folgende Funktionen stetig:
\begin{enumerate}
\item $f+g$
\item $\alpha f$ mit $\alpha\in\mathbb{R}$
\item $fg$
\item $(f\circ g)(x)=f(g(x))$
\end{enumerate}

\subsubsection{Wichtige stetige Funktionen}
\begin{enumerate}
\item Konstante Funktionen $f(x)=c$
\item Alle Polynome $p(x)$
\item Exponentialfunkion $e^x$
\item Eine rationale Funktion zweier Polynome mit $q(x)\neq 0$
\begin{equation*}
f(x)=\frac{p(x)}{q(x)}
\end{equation*}
\item $\sin(x)$ und $\cos(x)$
\end{enumerate}

\section{Folgen und Reihen in $\mathbb{C}$}

\subsection{Folgengrenzwerte in $\mathbb{C}$}
$\{z_n\}\subset \mathbb{C}$ konvergiert gegen $z\in\mathbb{C}$, falls gilt
\begin{equation*}
\lim\limits_{n\rightarrow \infty}|z_n-z|=0
\end{equation*}
Man berechnet hierbei die Konvergenzen von $Re(z_n)$ und $Im(z_n)$ und überprüft dann die obige Bedingung.

\subsection{Reihen in $\mathbb{C}$}
Sei $\{a_n\}\subset \mathbb{C}$ eine (komplexe) Folge. Die Reihe $\sum\limits_{n=0}^{\infty}a_n$ heißt absolut konvergent, falls die reelle Reihe $\sum\limits_{n=0}^{\infty}|a_n|$ konvergent ist.\\
Man berechnet also den Betrag der komplexen Folge und bestimmt dann Konvergenz und ggf. den Grenzwert der reellen Reihe.

\subsection{Potenzreihen}
Reihen der Form
\begin{equation*}
\sum\limits_{n=0}^{\infty}a_n(z-z_0)^n
\end{equation*}
mit reellen oder komplexen Koeffizienten $a_n$ und dem Zentrum $z_0\in\mathbb{C}$ wird Potenzreihe genannt.\\\\
Im Falle von höheren Potenzen, wie z.B.
\begin{equation*}
\sum\limits_{n=0}^{\infty}a_n(z-z_0)^{n^2}
\end{equation*}
kann das $n^2$ einfach substituiert werden (auch in $a_n$ substituieren!), um den Konvergenzradius zu bestimmen (danach keine Resubstitution!)
\subsubsection{Konvergenzradius $(-R,R)$ bestimmen}
\begin{equation*}
R=\lim\limits_{n\rightarrow\infty}\frac{|a_n|}{|a_{n+1}|}
\end{equation*}
\begin{equation*}
R=\lim\limits_{n\rightarrow\infty}\frac{1}{\sqrt[n]{|a_n|}}
\end{equation*}
Falls diese beiden Grenzwerte nicht existieren:
\begin{equation*}
R=\frac{1}{\limsup\limits_{n\rightarrow\infty}\sqrt[n]{|a_n|}}
\end{equation*}

\section{Differentialrechnung in $\mathbb{R}$}

\subsection{Rechenregeln}
Seien $f,g:D\subset \mathbb{R}\rightarrow\mathbb{R}$ zwei Funktionen, die an der Stelle $x_0\in D$ differenzierbar sind, dann gilt:
\begin{equation*}
(f+g)'(x_0)=f'(x_0)+g'(x_0)
\end{equation*}
Produktregel:
\begin{equation*}
(fg)'(x_0)=f'(x_0)g(x_0)+f(x_0)g'(x_0)
\end{equation*}
Quotientenregel:
\begin{equation*}
\left(\frac{f}{g}\right)'(x_0)=\frac{g(x_0)f'(x_0)-f(x_0)g'(x_0)}{g(x_0)^2}
\end{equation*}
Kettenregel:
\begin{equation*}
(g\circ f)'(x_0)=g'(f(x_0))f'(x_0)
\end{equation*}

\subsection{Differenzierbarkeit}
Eine Funktion $f(x)$ ist an der Stelle $x_0$ differenzierbar, falls folgender Grenzwert existiert:
\begin{equation*}
\lim\limits_{h\rightarrow 0}\frac{f(x_0+h)-f(x_0)}{h}
\end{equation*}
Aus Differenzierbarkeit folgt Stetigkeit.

\subsection{Extremstellen}
Notwendige Bedingung
\begin{equation*}
f'(x_0)=0
\end{equation*}
oder Randpunkte von $f$ oder Punkte, an denen $f$ nicht differenzierbar ist.\\\\
Hinreichende Bedingung für Minimum:
\begin{equation*}
f''(x_0)>0
\end{equation*}
oder Vorzeichenwechsel von $f'(x_0)$ von ''-'' nach ''+''\\\\
Hinreichende Bedingung für Maximum:
\begin{equation*}
f''(x_0)<0
\end{equation*}
oder Vorzeichenwechsel von ''+'' nach ''-''

\subsection{Satz von Rolle}
Sei $f:[a,b]\rightarrow\mathbb{R}$ stetig, auf dem offenen Intervall $(a,b)$ differenzierbar und es gelte $f(a)=f(b)$. Dann gibt es (mindestens) ein $x_0\in (a,b)$ mit $f'(x_0)=0$.

\subsection{Mittelwertsatz der Differentialrechnung}
Sei $f:[a,b]\rightarrow\mathbb{R}$ stetig und auf dem offenen Intervall $(a,b)$ differenzierbar. Dann gibt es (mindestens) ein $x_0\in(a,b)$ mit
\begin{equation*}
f'(x_0)=\frac{f(b)-f(a)}{b-a}
\end{equation*}

\subsection{Monotonie}
Sei $f:[a,b]\rightarrow \mathbb{R}$ stetig und auf dem offenen Intervall $(a,b)$ differenzierbar.\\
Die Funktion $f$ ist genau dann monoton wachsend, wenn
\begin{equation*}
f'(x_0)\geq 0
\end{equation*}
für alle $x\in(a,b)$.\\
Monoton fallend wird analog definiert.\\
Strenge Monotonie mit $<$ anstatt $\leq$.

\subsection{Krümmungsverhalten}
Sei $f:[a,b]\rightarrow\mathbb{R}$ stetig und auf dem offenen Intervall $(a,b)$ zweimal stetig differenzierbar.\\
Die Funktion $f$ ist genau dann konvex (linksgekrümmt), wenn
\begin{equation*}
f''(x)\geq 0
\end{equation*}
gilt und genau dann konkav (rechtsgekrümmt), wenn
\begin{equation*}
f''(x)\leq 0
\end{equation*}
erfüllt ist für alle $x\in (a,b)$.

\subsection{L'Hospital'sche Regel}
Seien $f,g:(a,b)\rightarrow\mathbb{R}$ zwei differenzierbare Funktionen mit
\begin{equation*}
\lim\limits_{x\rightarrow b}f(x)=\lim\limits_{x\rightarrow b}g(x)=0
\end{equation*}
oder
\begin{equation*}
\lim\limits_{x\rightarrow b}f(x)=\lim\limits_{x\rightarrow b}g(x)=\infty
\end{equation*}
so gilt:
\begin{equation*}
\lim\limits_{x\rightarrow b}\frac{f(x)}{g(x)}=\lim\limits_{x\rightarrow b}\frac{f'(x)}{g'(x)}
\end{equation*}

\subsection{Umkehrfunktion}
Sei $f:X\rightarrow Y$ eine bijektive Funktion. Mit Umkehrfunktion wird die Funktion $g:Y\rightarrow X$ bezeichnet.

\subsubsection{Begriffe für Abbildungen}
\textbf{Bild}\\
Das Bild der Funktion $f:X\rightarrow Y$ wird definiert als
\begin{equation*}
Bild(f)=f(X)=\{y\in Y|y=f(x),x\in X\}
\end{equation*}
\textbf{Surjektivität}\\
Eine Funktion $f:X\rightarrow Y$ heißt surjektiv, falls das Bild mit der Zielmenge $Y$ übereinstimmt (d.h. wenn jedes Element der Zielmenge $Y$ mindestens einmal als Funktionswert angenommen wird).\\\\
\textbf{Injektivität}\\
Eine Funktion $f:X\rightarrow Y$ heißt injektiv, falls keine zwei verschiedenen Elemente aus $X$ auf dasselbe Element aus $Y$ abgebildet werden.\\\\
\textbf{Bijektivität}\\
Eine Funktion $f:X\rightarrow Y$ heißt bijektiv, falls sie surjektiv und injektiv ist.

\subsubsection{Ableitung über die Umkehrfunktion}
\begin{equation*}
(f^{-1})'(y)=\frac{1}{f'(x)}=\frac{1}{f'(f^{-1}(y))}
\end{equation*}
Beispiel:
\begin{equation*}
f(x)=\arcsin(x)
\end{equation*}
\begin{equation*}
g(y)=\sin(y)
\end{equation*}
\begin{equation*}
g'(y)=\cos(y)
\end{equation*}
\begin{equation*}
f'(y)=\frac{1}{\cos(y)}
\end{equation*}
\begin{equation*}
f'(x)=\frac{1}{\cos(\arcsin(x))}=\frac{1}{\sqrt{1-\sin^2(\arcsin(x))}}=\frac{1}{\sqrt{1-x^2}}
\end{equation*}

\subsection{Wichtige Differentiale}
\begin{equation*}
(\sqrt{x})'=\frac{1}{2\sqrt{x}}
\end{equation*}
\begin{equation*}
(e^x)'=e^x
\end{equation*}
\begin{equation*}
(ln(x))'=\frac{1}{x}
\end{equation*}
\begin{equation*}
(\sin(x))'=\cos(x)
\end{equation*}
\begin{equation*}
(\cos(x))'=-\sin(x)
\end{equation*}
\begin{equation*}
(\tan(x))'=\frac{1}{\cos^2(x)}=1+\tan^2(x)
\end{equation*}
\begin{equation*}
(\cot(x))'=-\frac{1}{\sin^2(x)}
\end{equation*}
\begin{equation*}
(\arcsin(x))'=\frac{1}{\sqrt{1-x^2}}
\end{equation*}
\begin{equation*}
(\arccos(x))'=-\frac{1}{\sqrt{1-x^2}}
\end{equation*}
\begin{equation*}
(\arctan(x))'=\frac{1}{1+x^2}
\end{equation*}
\begin{equation*}
(arccot(x))'=-\frac{1}{1+x^2}
\end{equation*}
\begin{equation*}
(a^x)'=\ln(a)a^x
\end{equation*}
\begin{equation*}
(x^x)'=x^x(ln(x)+1)
\end{equation*}
\begin{equation*}
(arctanh(x))'=\frac{1}{1-x^2}
\end{equation*}
\begin{equation*}
(arcsinh(x))'=\frac{1}{\sqrt{x^2+1}}
\end{equation*}
\begin{equation*}
(arccosh(x))'=\frac{1}{\sqrt{x^2-1}}
\end{equation*}

\section{Integralrechnung in $\mathbb{R}$}
\subsection{Rechenregeln}
\begin{equation*}
\left|\int\limits_{a}^{b}f(x)dx\right|\leq \int\limits_{a}^{b}|f(x)|dx
\end{equation*}
\begin{equation*}
\int\limits_{a}^{b}(f(x)+g(x))dx=\int\limits_{a}^{b}f(x)dx+\int\limits_{a}^{b}g(x)dx
\end{equation*}
Für $\lambda\in\mathbb{R}$:
\begin{equation*}
\int\limits_{a}^{b}\lambda f(x)dx=\lambda\int\limits_{a}^{b}f(x)dx
\end{equation*}
Für $f(x)\leq g(x)$:
\begin{equation*}
\int\limits_{a}^{b}f(x)dx\leq\int\limits_{a}^{b}g(x)dx
\end{equation*}

\subsection{Mittelwertsatz der Integralrechnung}
Seien $f:[a,b]\rightarrow\mathbb{R}$ stetig, $g:[a,b]\rightarrow\mathbb{R}$ integrierbar und $g(x)\geq 0$ für alle $x\in [a,b]$. Dann gibt es ein $\xi\in[a,b]$ mit
\begin{equation*}
\int\limits_{a}^{b}f(x)g(x)dx=f(\xi)\int\limits_{a}^{b}g(x)dx
\end{equation*}

\subsection{Integrationsmethoden}

\subsubsection{Partielle Integration}
Seien $u,v:[a,b]\rightarrow\mathbb{R}$ zwei differenzierbare Funktionen. Es gilt:
\begin{equation*}
\int u'(x)v(x)=u(x)v(x)-\int u(x)v'(x)
\end{equation*}

\subsubsection{Substitution}
Sei $f:[c,d]\rightarrow\mathbb{R}$ eine stetige Funktion, $F:[c,d]\rightarrow\mathbb{R}$ eine Stammfunktion von $f$ und $g:[a,b]\rightarrow[c,d]$ eine differenzierbare Funktion. Es gilt:
\begin{equation*}
\int f(g(x))g'(x)dx=F(g(x))+c
\end{equation*}
Für das bestimmte Integral gilt entsprechend:
\begin{equation*}
\int\limits_{a}^{b}f(g(x)g'(x)dx=F(g(x))|_a^b=\int\limits_{g(a)}^{g(b)}f(y)dy
\end{equation*}

\subsubsection{Partialbruchzerlegung}
Bekannte Spezialfälle:
\begin{equation*}
\int \frac{1}{x-a}dx=\ln|x-a|+c
\end{equation*}
\begin{equation*}
\int \frac{1}{(x-a)^n}dx=-\frac{1}{n-1}\frac{1}{(x-a)^{n-1}}\;falls\;n\geq 2
\end{equation*}
\begin{equation*}
\int \frac{1}{(x-a)^2+b^2}dx=\frac{1}{b}\arctan\frac{x-a}{b}+c\;falls\;b>0
\end{equation*}
\begin{equation*}
\frac{x-a}{(x-a)^2+b^2}dx=\frac{1}{2}\ln((x-a)^2+b^2)+c
\end{equation*}
Ziel ist es, das Integral $\int \frac{p(x)}{q(x)}dx$ auf eines dieser vier Integrale zurückzuführen. Dabei geht man wie folgt vor:
\begin{enumerate}
\item Ist der Grad des Zählers $p(x)$ größer oder gleich dem Grad des Nenners $q(x)$, so führt man die Polynomdivision durch und erhält die Darstellung
\begin{equation*}
f(x)=p_0(x)+\frac{p_1(x)}{q(x)}
\end{equation*}
wobei der Grad von $p_1(x)$ kleiner ist, als der Grad von $q(x)$.
\item Man sucht alle reellen Nullstellen des Nenners $q(x)$ und zerlegt
\begin{equation*}
\begin{split}q(x)=c(x-b_1)^{k_1}(x-b_2)^{k_2}...(x-b_r)^{k_r} \\ \cdot q_1(x)^{l_1}q_2(x)^{l_2}...q_s(x)^{l_s} \end{split}
\end{equation*}
wobei $q_i$ quadratische Polynome sind, die nicht in Linearfaktoren zerlegt werden können. Sie haben die Gestalt
\begin{equation*}
q_i=(x-a_i)^2+{d_i}^2
\end{equation*}
\item Jetzt stellt man die Funktion $\frac{p(x)}{q(x)}$ als Summe von Funktionen der Form
\begin{equation*}
\frac{A_1}{x-b},\frac{A_2}{(x-b)^2},...,\frac{A_k}{(x-b)^k}
\end{equation*}
und
\begin{equation*}
\frac{B_1x+C_1}{Q(x)},\frac{B_2x+C_2}{Q(x)^2},...,\frac{B_lx+C_l}{Q(x)^l}
\end{equation*}
dar, wobei $b\in \{b_1,b_2,...,b_r\},Q\in\{q_1,q_2,...,q_s\}$ und die Koeffizienten $A_i, B_i, C_i$ zu bestimmen sind.
\item Im letzten Schritt berechnet man die Stammfunktion der einzelnen Summanden (Partialbrüche). Für die Brüche mit $Q(x)^l,l>1$ verwendet man rekursive Formeln (siehe Formelsammlungen).
\end{enumerate}

\subsubsection{Integration von Potenzreihen}
Sei $f(x)=\sum\limits_{n=0}^{\infty}c_nx^n$ eine durch eine Potenzreihe mit Konvergenzradius $R>0$ definierte Funktion, dann gilt die auf $(-R,R)$ definierte Stammfunktion
\begin{equation*}
F(x)=\sum\limits_{n=0}^{\infty}\frac{c_n}{n+1}x^{n+1}
\end{equation*}

\subsection{Uneigentliche Integrale}
Integrale, bei denen eine Integrationsgrenze im Unendlichen liegt oder eine Singularität ist.
\begin{equation*}
\int\limits_{a}^{\infty}f(x)dx=\lim\limits_{M\rightarrow\infty}\int\limits_{a}^{M}f(x)dx
\end{equation*}
\begin{equation*}
\int\limits_{a}^{b}f(x)dx=\lim\limits_{\epsilon\rightarrow 0^+}\int\limits_{a+\epsilon}^{b}f(x)dx
\end{equation*}
Diese Integrale heißen konvergent, falls der Grenzwert endlich ist.\\\\
\textbf{Vergleichskriterium bei uneigentlichen Integralen:}\\
Seien $f,g: [a,\infty) \rightarrow \mathbb{R}$ zwei Funktionen, die auf jedem endlichen Intervall $[a,M]$ integrierbar sind. Es gelte
\begin{equation*}
|f(x)|\leq g(x)\;\forall\;x\in [a,\infty)
\end{equation*}
dann folgt aus der Konvergenz des Integrals
\begin{equation*}
\int\limits_{a}^{\infty}g(x)dx
\end{equation*}
die Konvergenz des Integrals
\begin{equation*}
\int\limits_{a}^{\infty}f(x)dx
\end{equation*}
\textbf{Zusammenhang zwischen Konvergenz von Reihen und uneigentlichen Integralen:}\\
Es sei $f:[1,\infty) \rightarrow\mathbb{R}$ eine nichtnegative, monoton fallende Funktion. Dann ist die Reihe
\begin{equation*}
\sum\limits_{n=1}^{\infty}f(x)
\end{equation*}
genau dann konvergent, wenn folgendes uneigentliche Integral konvergent ist:
\begin{equation*}
\int\limits_{1}^{\infty}f(x)dx
\end{equation*}
(Die Werte sind im Allgemeinen unterschiedlich!)\\
\textbf{Für Unendlich an beiden Integrationsgrenzen:}\\
Sei $f:\mathbb{R}\rightarrow\mathbb{R}$ eine Funktion, die auf jedem abgeschlossenen Intervall $I\in \mathbb{R}$ integrierbar ist. Man definiert dann
\begin{equation*}
\int\limits_{-\infty}^{\infty}f(x)dx=\int\limits_{-\infty}^{a}f(x)dx+\int\limits_{a}^{\infty}f(x)dx
\end{equation*}
falls beide uneigentliche Integrale auf der rechten Seite konvergieren. In diesem Fall hängt das Ergebnis nicht von der Wahl der Zahl $a\in \mathbb{R}$ ab.\\
\textbf{Cauchy-Hauptwert:}\\
Sei $f:\mathbb{R}\rightarrow\mathbb{R}$ eine Funktion, die auf dem Intervall $[-M,M]$ für alle $M\in\mathbb{R},M>0$ integrierbar ist. Der Cauchy-Hauptwert ist gegeben als
\begin{equation*}
CHW\;\int\limits_{-\infty}^{\infty}f(x)dx=\lim\limits_{M\rightarrow\infty}\int\limits_{-M}^{M}f(x)dx
\end{equation*}
falls der Grenzwert existiert.\\
Falls das uneigentliche Integral $\int\limits_{-\infty}^{\infty}f(x)dx$ konvergent ist, existiert auch der Cauchy-Hauptwert, der dann mit dem Wert des uneigentlichen Integrals übereinstimmt (Umkehrung gilt nicht!).

\subsection{Laplace-Transformation}
Sei $f: [0,\infty) \rightarrow\mathbb{R}$ eine auf jedem endlichen Intervall $[0,M]$ integrierbare Funkion, dann heißt
\begin{equation*}
F(s)=({\cal L}f)(s)=\int\limits_{0}^{\infty}e^{-st}f(t)dt
\end{equation*}
Laplace-Transformierte der Funktion $f$. (Definiert für alle $s\geq 0$, für die das uneigentliche Integral konvergiert.)\\\\
\textbf{Exponentielle Ordnung:}\\
Die Funktion $f:[0,\infty)\rightarrow\mathbb{R}$ ist von exponentieller Ordnung $\gamma$ (mit $\gamma\in\mathbb{R}$), falls eine Konstante $C\geq 0$ existiert, sodass gilt:
\begin{equation*}
|f(t)|\leq Ce^{\gamma t}\;\forall\;t\in[0,\infty)
\end{equation*}
\textbf{Existenz einer Laplace-Transformierten:}\\
Sei $f:[0,\infty)\rightarrow\mathbb{R}$ eine stückweise stetige Funktion von exponentieller Ordnung $\gamma$. Dann existiert die Laplace-Transformierte ${\cal L}f$ für alle $s>\gamma$.\\\\
\textbf{Wichtige Laplace-Transformierte:}\\
\begin{equation*}
f(t)=1\Rightarrow F(s)={\cal L}(f)(s)=\frac{1}{s},s>0
\end{equation*}
\begin{equation*}
f(t)=e^{kt}\Rightarrow F(s)={\cal L}(f)(s)=\frac{1}{s-k},s>k
\end{equation*}
\begin{equation*}
f(t)=\cos(\omega t)\Rightarrow F(s)={\cal L}(f)(s)=\frac{s}{s^2+\omega^2},s>0
\end{equation*}
\begin{equation*}
f(t)=\sin(\omega t)\Rightarrow F(s)={\cal L}(f)(s)=\frac{\omega}{s^2+\omega^2}, s>0
\end{equation*}
\textbf{\textbf{Von Laplace-Transformierter auf $f(x)$ schließen (Bsp.):}}\\
\begin{equation*}
{\cal L}(f)(s)=\frac{1}{s(s+1)}
\end{equation*}
\begin{equation*}
{\cal L}(f)(s)=\frac{1}{s}-\frac{1}{s+1}={\cal L}(1-e^{-t})
\end{equation*}
\textbf{Rechenregeln:}\\
\begin{equation*}
{\cal L}(af+bg)=a{\cal L}(f)+b{\cal L}(g)
\end{equation*}
\begin{equation*}
{\cal L}(f')(s)=s{\cal L}(f)(s)-f(0)
\end{equation*}
\begin{equation*}
{\cal L}(f'')(s)=s^2{\cal L}(f)(s)-f'(0)-sf(0)
\end{equation*}

\subsection{Wichtige Integrale}
\begin{equation*}
\int x^n dx=\frac{x^{n+1}}{n+1}+c
\end{equation*}
\begin{equation*}
\int \frac{f'(x)}{f(x)}dx=\ln|f(x)|+c
\end{equation*}
\begin{equation*}
\int a^x dx=\frac{a^x}{\ln(a)}+c
\end{equation*}
\begin{equation*}
\int \ln(x)dx=-x+x\ln(x)+c
\end{equation*}
\begin{equation*}
\int\frac{1}{x^{\alpha}}dx=\begin{cases}\ln|x|+c & falls\;\alpha =1 \\ \frac{1}{1-\alpha}x^{1-\alpha} & falls\;\alpha\neq 1\end{cases}
\end{equation*}
\begin{equation*}
\int\limits_{0}^{1}\frac{1}{x^{\alpha}}dx=\begin{cases}\frac{1}{1-\alpha} & falls\;\alpha<1 \\ \infty & falls\;\alpha>1\end{cases}
\end{equation*}
\begin{equation*}
\int\limits_{1}^{\infty}\frac{1}{x^{\alpha}}dx=\begin{cases}\frac{1}{\alpha-1} & falls\;\alpha>1 \\ \infty & falls\;\alpha <1\end{cases}
\end{equation*}

\section{Sonstiges}

\subsection{Sinus- und Kosinus Hyperbolicus}
\begin{equation*}
\sinh(x)=\frac{e^x-e^{-x}}{2}=-i\sin(ix)
\end{equation*}
Der Sinus Hyperbolicus ist eine ungerade Funktion.
\begin{equation*}
\cosh(x)=\frac{e^x+e^{-x}}{2}=\cos(ix)
\end{equation*}
Der Kosinus Hyperbolicus ist eine gerade Funktion.
\begin{equation*}
\cosh^2(x)-\sinh^2(x)=1
\end{equation*}
\begin{equation*}
(\sinh(x))'=\cosh(x)
\end{equation*}
\begin{equation*}
(\cosh(x))'=\sinh(x)
\end{equation*}
\\\\
Diese Formelsammlung ist eine überarbeitete und erweiterte Version der ''Formelsammlung Mathematik 1 für Elektroingenieure'' von Sebastian Wagner.\\\\
Lizenz: CC BY-NC-SA 3.0\\
\url{http://creativecommons.org/licenses/by-nc-sa/3.0/de/}

\end{document}








